ğŸ” Sistema de Consulta Documental Inteligente (RAG con Ollama)
ImplementaciÃ³n prÃ¡ctica de Retrieval-Augmented Generation (RAG) con modelos de lenguaje locales para consultas documentales automatizadas.

ğŸ“‹ DescripciÃ³n
Este proyecto implementa un sistema completo de RAG (Retrieval-Augmented Generation) que permite realizar consultas inteligentes sobre documentos de texto. Combina embeddings semÃ¡nticos con modelos de lenguaje locales (via Ollama) para proporcionar respuestas precisas basadas en el contenido de los documentos.

ğŸš€ CaracterÃ­sticas Principales
ğŸ” BÃºsqueda SemÃ¡ntica: IndexaciÃ³n de documentos usando embeddings (all-MiniLM-L6-v2)

ğŸ¤– Modelos Locales: IntegraciÃ³n con Ollama para ejecutar modelos LLM localmente (Llama3, Mistral, etc.)

ğŸ“š Procesamiento MultilingÃ¼e: Soporte para documentos en espaÃ±ol con manejo robusto de encoding

âš¡ RÃ¡pido y Eficiente: BÃºsqueda por similitud con FAISS para recuperaciÃ³n rÃ¡pida

ğŸ§  Contexto Relevante: Recupera solo los fragmentos mÃ¡s relevantes para cada consulta

ğŸ”§ Configurable: ParÃ¡metros ajustables para fragmentaciÃ³n, modelos y resultados

ğŸ› ï¸ Requisitos Previos
1. Instalar Python 3.8+
bash
python --version
2. Instalar Ollama
bash
# Descargar e instalar desde: https://ollama.com/
# Luego, descargar un modelo:
ollama pull llama3
ollama pull mistral
3. Instalar Dependencias de Python
bash
pip install sentence-transformers faiss-cpu numpy tqdm
ğŸ“¦ InstalaciÃ³n
Clonar o descargar el proyecto

bash
git clone <repository-url>
cd rag-ollama-system
Instalar dependencias

bash
pip install -r requirements.txt
Preparar documentos

Colocar los documentos .txt en la carpeta especificada en RUTA_DOCUMENTOS

Por defecto: E:/maestria en bd y bi/Text Mining y procesamiento de lenguaje natural/tarea4/docs

ğŸ¯ Uso
EjecuciÃ³n BÃ¡sica
bash
python rag_ollama.py
Opciones de LÃ­nea de Comandos
bash
# Especificar modelo diferente
python rag_ollama.py --modelo mistral

# Cambiar nÃºmero de resultados
python rag_ollama.py --resultados 6

# Especificar ruta de documentos personalizada
python rag_ollama.py --docs "ruta/a/mis/documentos"
Ejemplo de SesiÃ³n Interactiva
bash
SISTEMA DE CONSULTA DOCUMENTAL - Modelo: llama3
Escriba su pregunta o 'salir' para terminar
======================================================================

ğŸ’¬ CONSULTA > Â¿QuÃ© es Webhomo?

ğŸ” Buscando informaciÃ³n relevante...
ğŸ¤– Consultando modelo llama3...

ğŸ“„ RESPUESTA:
SegÃºn Documento 1, Webhomo es una herramienta altamente calificada entre 78 herramientas 
para anotaciÃ³n manual de documentos en NLP y text mining.

ğŸ’¬ CONSULTA > salir
ğŸ“ Estructura del Proyecto
text
rag-ollama-system/
â”œâ”€â”€ rag_ollama.py          # Script principal del sistema
â”œâ”€â”€ requirements.txt       # Dependencias de Python
â”œâ”€â”€ docs/                  # Carpeta con documentos .txt
â”‚   â”œâ”€â”€ documento1.txt
â”‚   â”œâ”€â”€ documento2.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md             # Este archivo
ğŸ—ï¸ Arquitectura TÃ©cnica
Componentes del Sistema
Cargador de Documentos: Lee archivos .txt con manejo de encoding (UTF-8, Latin-1)

Fragmentador: Divide documentos en fragmentos manejables (~800 caracteres)

Motor de Embeddings: Usa Sentence Transformers para crear representaciones vectoriales

Ãndice FAISS: Almacena y busca embeddings por similitud coseno

IntegraciÃ³n Ollama: Conecta con modelos LLM locales para generaciÃ³n

Sistema RAG: Combina recuperaciÃ³n y generaciÃ³n con plantilla estructurada

Flujo del Sistema
text
Documentos â†’ FragmentaciÃ³n â†’ Embeddings â†’ FAISS Index â†’ BÃºsqueda â†’ Contexto â†’ Ollama â†’ Respuesta
âš™ï¸ ConfiguraciÃ³n
Variables Principales en el CÃ³digo
python
# Modelo de embeddings (puede cambiarse por otros de Sentence Transformers)
MODELO_EMBEDDINGS = "all-MiniLM-L6-v2"

# Modelo de Ollama por defecto
MODELO_LLM_PREDETERMINADO = "llama3"

# Ruta a documentos (ajustar segÃºn tu sistema)
RUTA_DOCUMENTOS = "ruta/a/tus/documentos"

# TamaÃ±o mÃ¡ximo de fragmentos (caracteres)
tamano_fragmento = 800

# NÃºmero de fragmentos a recuperar
resultados_maximos = 4
ğŸ“Š Ejemplos de Consultas
El sistema puede manejar diversos tipos de consultas:

Consultas Factuales

text
Â¿CuÃ¡ntas herramientas fueron evaluadas en el estudio?
Consultas Comparativas

text
Â¿CuÃ¡les son las diferencias entre GPT y GPT-3?
Consultas de SÃ­ntesis

text
Resume los criterios de evaluaciÃ³n de herramientas de anotaciÃ³n
Consultas EspecÃ­ficas

text
Â¿QuÃ© tareas puede realizar GPT-3 segÃºn los documentos?
âš ï¸ Limitaciones Conocidas
Dependencia de Calidad de Documentos: Respuestas precisas requieren documentos bien estructurados

TamaÃ±o de Contexto: Limitado por el tamaÃ±o de ventana del modelo LLM

Modelos Locales: Requieren hardware adecuado (CPU/GPU) para buen rendimiento

Lenguaje: Optimizado para espaÃ±ol, pero funciona con mÃºltiples idiomas

ValidaciÃ³n: No incluye validaciÃ³n automÃ¡tica de hechos en respuestas

ğŸš€ Mejoras Futuras
Interfaz web con Gradio o Streamlit

Soporte para mÃ¡s formatos (PDF, DOCX, HTML)

Cache de embeddings para documentos grandes

MÃ©tricas de evaluaciÃ³n de precisiÃ³n

Soporte para mÃºltiples modelos de embeddings

Sistema de historial de conversaciones
